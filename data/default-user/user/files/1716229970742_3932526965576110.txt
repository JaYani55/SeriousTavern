# This is an example of a simple scraper for job-listings.

from selenium import webdriver
import pandas as pd

driver = webdriver.Chrome()
# Initialize the Chrome WebDriver
# driver = webdriver.Chrome(executable_path="C:\\Users\\Janal\\Downloads\\chromedriver_win32\\chromedriver.exe")
# Open the webpage
driver.get("https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html")
elements = driver.find_elements("xpath", '//*[@id="eintraege"]//a')
data = []

    # Iterate over the rows in pairs (job title, date, URL)
for element in elements:
        job_title = element.text.split('\n')[0]
        date = element.text.split('\n')[1] if '\n' in element.text else None
        url = element.get_attribute('href')
        data.append([job_title, date, url])
# Create a DataFrame from the data
df = pd.DataFrame(data, columns=['Job Title', 'Date', 'URL'])
print(df)
# Save the DataFrame to a CSV file
df.to_csv('schwarzesbrett.csv', index=False)

driver.quit()

# ToDO: Post Limit exceeded. Also, crawl to update?
# Deleted Bearertoken for security reasons. Remember to add it again before running the script.